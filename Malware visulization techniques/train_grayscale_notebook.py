# -*- coding: utf-8 -*-
"""train_grayscale_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qf0H1OIGLhxgbT4Pqjmyg-VXpmtDEpad
"""

import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
from PIL import Image
import time
import wandb
from codecarbon import EmissionsTracker

# Initialize Wandb
wandb.init(project="test", group="Model Comparison", name="GoogLeNet")

# Define your category
classes = ['ganelp', 'sfone', 'upatre', 'wabot', 'wacatac']

# Define device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



# Define transform without normalization
basic_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),  # Convert grayscale image to 3 channels
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
])

# Define path and load basic dataset
path = '/content/drive/MyDrive/BODMAS_grayscale'

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes, self.class_to_idx = self._find_classes(root_dir)
        self.samples = self.make_dataset(root_dir, self.class_to_idx)

    def _find_classes(self, dir):
        classes = [d.name for d in os.scandir(dir) if d.is_dir()]
        classes.sort()
        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
        return classes, class_to_idx

    def make_dataset(self, dir, class_to_idx):
        images = []
        dir = os.path.expanduser(dir)
        for target in sorted(class_to_idx.keys()):
            d = os.path.join(dir, target)
            if not os.path.isdir(d):
                continue
            for root, _, fnames in sorted(os.walk(d)):
                for fname in sorted(fnames):
                    path = os.path.join(root, fname)
                    item = (path, class_to_idx[target])
                    images.append(item)
        return images

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        path, target = self.samples[index]
        image = Image.open(path).convert('RGB')
        if self.transform is not None:
            image = self.transform(image)
        return image, target

# Instantiate basic dataset with basic_transform to calculate mean and std
basic_dataset = MyDataset(root_dir=path, transform=basic_transform)

# Calculate mean and std
mean = 0.
std = 0.
for images, _ in DataLoader(basic_dataset, batch_size=32, shuffle=False):
    batch_samples = images.size(0)
    images = images.view(batch_samples, images.size(1), -1)
    mean += images.mean(2).sum(0)
    std += images.std(2).sum(0)

mean /= len(basic_dataset)
std /= len(basic_dataset)

params = {
    'model': 'googlenet',  # or 'inception_v3', 'mobilenet_v2'
    'batch_size': 32,
    'lr': 0.001,
    'momentum': 0.9,
    'epochs': 100,
    'patience': 10
}

if params['model'] == 'inception_v3':
    size = 299
else:
    size = 224

# Define actual transform using calculated mean and std
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.RandomResizedCrop(size),
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std),
])

# Reload dataset with actual transform using MyDataset class
dataset = MyDataset(root_dir=path, transform=transform)

# Split dataset into training and validation
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

if params['model'] == 'inception_v3':
    model = models.inception_v3(pretrained=False)
    model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, len(classes))
    model.fc = nn.Linear(model.fc.in_features, len(classes))
elif params['model'] == 'googlenet':
    model = models.googlenet(pretrained=False)
    model.fc = nn.Linear(model.fc.in_features, len(classes))
elif params['model'] == 'mobilenet_v2':
    model = models.mobilenet_v2(pretrained=False)
    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(classes))

model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'])

# Define data loader using params['batch_size']
train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])

# Monitor models, loss functions, and optimizers on Wandb
wandb.watch(model, log="all")

# Instantiate a Carbon Tracker
tracker = EmissionsTracker()

# Start Carbon Tracking
tracker.start()

# Train the model with early stopping
num_epochs = params['epochs']

    # early stopping
patience = params['patience']
no_improve_epoch = 0
best_val_loss = float('inf')

for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        running_corrects = 0
        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            if params['model'] == 'inception_v3':
                outputs, aux_outputs = model(inputs)
                loss1 = criterion(outputs, labels)
                loss2 = criterion(aux_outputs, labels)
                loss = loss1 + 0.4 * loss2  # according to the paper
                _, preds = torch.max(outputs, 1)
            elif params['model'] == 'googlenet':
                outputs = model(inputs)
                loss = criterion(outputs.logits, labels)
                _, preds = torch.max(outputs.logits, 1)
            elif params['model'] == 'mobilenet_v2':
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        epoch_loss = running_loss / len(train_dataset)
        epoch_acc = running_corrects.double() / len(train_dataset)
        print(f'Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')

        wandb.log({'Train Loss': epoch_loss, 'Train Acc': epoch_acc.item()})

        # Validation
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                if params['model'] == 'inception_v3':
                  outputs = model(inputs)
                  if isinstance(outputs, tuple):
                    outputs, aux_outputs = outputs
                    loss1 = criterion(outputs, labels)
                    loss2 = criterion(aux_outputs, labels)
                    loss = loss1 + 0.4 * loss2
                  else:
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                elif params['model'] == 'googlenet':
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                elif params['model'] == 'mobilenet_v2':
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                val_loss += loss.item() * inputs.size(0)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        val_loss /= total
        val_acc = correct / total
        print(f'Validation Loss: {val_loss:.4f}, Acc: {val_acc:.4f}')

        wandb.log({'Validation Loss': val_loss, 'Validation Acc': val_acc})

        # Check for early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            no_improve_epoch = 0
            torch.save({
              'epoch': epoch,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'parameters': params
            }, 'model.pth')
        else:
            no_improve_epoch += 1

        if no_improve_epoch >= patience:
            print('Early stopping...')
            break

# Stop carbon tracking, and output the results
tracker.stop()
final_emissions_data = tracker.final_emissions_data
final_emissions = tracker.final_emissions

print("Final emissions data: ", final_emissions_data)
print("Final emissions: ", final_emissions)


# Log to Wandb
wandb.log({"Total emissions": final_emissions})

# End Wandb running
wandb.finish()