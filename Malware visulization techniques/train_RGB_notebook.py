# -*- coding: utf-8 -*-
"""RGB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wvmpRMDXMV2pwoomp2rBGG3DaBnhP-es
"""

import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
from PIL import Image
import time
import wandb
from codecarbon import EmissionsTracker

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"


# Initialize Wandb
wandb.init(project="test", group="Model Comparison", name="GoogLeNet")

# Define your category
classes = ['ganelp', 'sfone', 'upatre', 'wabot', 'wacatac']

# Define device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define transform without normalization
basic_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
])

# Define path and load basic dataset
path = '/content/drive/MyDrive/BODMAS_RGB'

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes, self.class_to_idx = self._find_classes(root_dir)
        self.samples = self.make_dataset(root_dir, self.class_to_idx)

    def _find_classes(self, dir):
        classes = [d.name for d in os.scandir(dir) if d.is_dir()]
        classes.sort()
        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
        return classes, class_to_idx

    def make_dataset(self, dir, class_to_idx):
        images = []
        dir = os.path.expanduser(dir)
        for target in sorted(class_to_idx.keys()):
            d = os.path.join(dir, target)
            if not os.path.isdir(d):
                continue
            for root, _, fnames in sorted(os.walk(d)):
                for fname in sorted(fnames):
                    path = os.path.join(root, fname)
                    item = (path, class_to_idx[target])
                    images.append(item)
        return images

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        path, target = self.samples[index]
        image = Image.open(path)  # 虽然您的图像已经是RGB的，但这步可以确保它们都是三通道的
        if self.transform is not None:
            image = self.transform(image)
        return image, target

# Instantiate basic dataset with basic_transform to calculate mean and std
basic_dataset = MyDataset(root_dir=path, transform=basic_transform)

# Calculate mean and std
mean = 0.
std = 0.
for images, _ in DataLoader(basic_dataset, batch_size=32, shuffle=False):
    batch_samples = images.size(0)
    images = images.view(batch_samples, images.size(1), -1)
    mean += images.mean(2).sum(0)
    std += images.std(2).sum(0)

mean /= len(basic_dataset)
std /= len(basic_dataset)

# Define actual transform using calculated mean and std
transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std),
])

# Reload dataset with actual transform using MyDataset class
dataset = MyDataset(root_dir=path, transform=transform)

# Split dataset into training and validation
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Define data loader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# Define model
model = models.googlenet(pretrained=False)
model.fc = nn.Linear(model.fc.in_features, len(classes))
model = model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Monitor models, loss functions, and optimizers on Wandb
wandb.watch(model, log="all")

# Instantiate a Carbon Tracker
tracker = EmissionsTracker()

# Start Carbon Tracking
tracker.start()

# Train the model with early stopping
num_epochs = 100
patience = 10  # early stop patience
best_val_loss = None
no_improve_count = 0

for epoch in range(num_epochs):
    start_time = time.time()
    model.train()

    train_loss = 0.0
    train_correct = 0
    train_total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        loss = criterion(outputs.logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        _, predicted = outputs.logits.max(1)
        train_total += labels.size(0)
        train_correct += predicted.eq(labels).sum().item()

    train_loss /= len(train_loader)  # Average Training Loss
    train_accuracy = 100 * train_correct / train_total

    # Here, we record the training loss and the current learning rate
    wandb.log({"Train Loss": train_loss, "Learning Rate": optimizer.param_groups[0]['lr'], "Train Accuracy": train_accuracy})

    # validation model
    model.eval()
    val_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    val_loss /= len(val_loader)  # Average Validation Loss
    val_accuracy = 100 * correct / total

    print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy}, Train Loss: {train_loss}, Val Accuracy: {val_accuracy}, Val Loss: {val_loss}')

    # Record performance metrics to Wandb
    wandb.log({"Validation Loss": val_loss, "Validation Accuracy": val_accuracy})

    # Calculate and record the training time for each epoch
    end_time = time.time()
    epoch_time = end_time - start_time
    wandb.log({"Time per epoch": epoch_time})

    # check early stop
    if best_val_loss is None or val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), '/content/drive/MyDrive/googlenet.pth')
        no_improve_count = 0
    else:
        no_improve_count += 1
        if no_improve_count >= patience:
            print('early-stopping')
            break

# Stop carbon tracking, and output the results
tracker.stop()
final_emissions_data = tracker.final_emissions_data
final_emissions = tracker.final_emissions

print("Final emissions data: ", final_emissions_data)
print("Final emissions: ", final_emissions)


# Log to Wandb
wandb.log({"Total emissions": final_emissions})

# End Wandb running
wandb.finish()

pip install wandb

pip install wandb codecarbon

import os
import numpy as np
from PIL import Image
import math

def convert_to_rgb_image(x: np.array) -> np.array:
    width = 1920  # 固定宽度为1920像素
    height = math.ceil(len(x) / (width * 3))
    img = np.zeros((height, width, 3), dtype=np.uint8)
    for i in range(height):
        for j in range(width):
            idx = i * width * 3 + j * 3
            if idx + 2 < len(x):
                img[i, j, :] = x[idx:idx+3]
    return img

input_folders = [
    '/content/drive/Shareddrives/BODMAS/BODMAS/ganelp',
    '/content/drive/Shareddrives/BODMAS/BODMAS/sfone',
    '/content/drive/Shareddrives/BODMAS/BODMAS/upatre',
    '/content/drive/Shareddrives/BODMAS/BODMAS/wabot',
    '/content/drive/Shareddrives/BODMAS/BODMAS/wacatac'
]

output_folders = [
    '/content/drive/MyDrive/BODMAS_RGB_NEW/ganelp',
    '/content/drive/MyDrive/BODMAS_RGB_NEW/sfone',
    '/content/drive/MyDrive/BODMAS_RGB_NEW/upatre',
    '/content/drive/MyDrive/BODMAS_RGB_NEW/wabot',
    '/content/drive/MyDrive/BODMAS_RGB_NEW/wacatac'
]

for in_folder, out_folder_rgb in zip(input_folders, output_folders):
    files = os.listdir(in_folder)
    for i, file_name in enumerate(files):
        if i >= 1000:
            break
        file_path = os.path.join(in_folder, file_name)
        with open(file_path, 'rb') as f:
            x = np.frombuffer(f.read(), dtype=np.uint8)

        # 转换为RGB图像
        img_rgb = convert_to_rgb_image(x)
        img_rgb = Image.fromarray(img_rgb)
        out_path_rgb = os.path.join(out_folder_rgb, file_name + '.png')
        img_rgb.save(out_path_rgb)

print("转换完成!")

import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
from PIL import Image
import time
import wandb
from codecarbon import EmissionsTracker

# Initialize Wandb
wandb.init(project="test", group="Model Comparison", name="GoogLeNet")

# Define your category
classes = ['ganelp', 'sfone', 'upatre', 'wabot', 'wacatac']

# Define device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



# Define transform without normalization
basic_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
])

# Define path and load basic dataset
path = '/content/drive/MyDrive/BODMAS_RGB'

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes, self.class_to_idx = self._find_classes(root_dir)
        self.samples = self.make_dataset(root_dir, self.class_to_idx)

    def _find_classes(self, dir):
        classes = [d.name for d in os.scandir(dir) if d.is_dir()]
        classes.sort()
        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
        return classes, class_to_idx

    def make_dataset(self, dir, class_to_idx):
        images = []
        dir = os.path.expanduser(dir)
        for target in sorted(class_to_idx.keys()):
            d = os.path.join(dir, target)
            if not os.path.isdir(d):
                continue
            for root, _, fnames in sorted(os.walk(d)):
                for fname in sorted(fnames):
                    path = os.path.join(root, fname)
                    item = (path, class_to_idx[target])
                    images.append(item)
        return images

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        path, target = self.samples[index]
        image = Image.open(path).convert('RGB')
        if self.transform is not None:
            image = self.transform(image)
        return image, target

# Instantiate basic dataset with basic_transform to calculate mean and std
basic_dataset = MyDataset(root_dir=path, transform=basic_transform)

# Calculate mean and std
mean = 0.
std = 0.
for images, _ in DataLoader(basic_dataset, batch_size=32, shuffle=False):
    batch_samples = images.size(0)
    images = images.view(batch_samples, images.size(1), -1)
    mean += images.mean(2).sum(0)
    std += images.std(2).sum(0)

mean /= len(basic_dataset)
std /= len(basic_dataset)

params = {
    'model': 'inception_v3',  # or 'inception_v3', 'mobilenet_v2'
    'batch_size': 32,
    'lr': 0.001,
    'momentum': 0.9,
    'epochs': 100,
    'patience': 10
}

if params['model'] == 'inception_v3':
    size = 299
else:
    size = 224

# Define actual transform using calculated mean and std
transform = transforms.Compose([
    transforms.RandomResizedCrop(size),
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std),
])

# Reload dataset with actual transform using MyDataset class
dataset = MyDataset(root_dir=path, transform=transform)

# Split dataset into training and validation
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

if params['model'] == 'inception_v3':
    model = models.inception_v3(pretrained=False)
    model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, len(classes))
    model.fc = nn.Linear(model.fc.in_features, len(classes))
elif params['model'] == 'googlenet':
    model = models.googlenet(pretrained=False)
    model.fc = nn.Linear(model.fc.in_features, len(classes))
elif params['model'] == 'mobilenet_v2':
    model = models.mobilenet_v2(pretrained=False)
    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(classes))

model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'])

# Define data loader using params['batch_size']
train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])

# Monitor models, loss functions, and optimizers on Wandb
wandb.watch(model, log="all")

# Instantiate a Carbon Tracker
tracker = EmissionsTracker()

# Start Carbon Tracking
tracker.start()

# Train the model with early stopping
num_epochs = params['epochs']

    # early stopping
patience = params['patience']
no_improve_epoch = 0
best_val_loss = float('inf')

for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        running_corrects = 0
        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            if params['model'] == 'inception_v3':
                outputs, aux_outputs = model(inputs)
                loss1 = criterion(outputs, labels)
                loss2 = criterion(aux_outputs, labels)
                loss = loss1 + 0.4 * loss2  # according to the paper
                _, preds = torch.max(outputs, 1)
            elif params['model'] == 'googlenet':
                outputs = model(inputs)
                loss = criterion(outputs.logits, labels)
                _, preds = torch.max(outputs.logits, 1)
            elif params['model'] == 'mobilenet_v2':
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        epoch_loss = running_loss / len(train_dataset)
        epoch_acc = running_corrects.double() / len(train_dataset)
        print(f'Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')

        wandb.log({'Train Loss': epoch_loss, 'Train Acc': epoch_acc.item()})

        # Validation
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                if params['model'] == 'inception_v3':
                  outputs = model(inputs)
                  if isinstance(outputs, tuple):
                    outputs, aux_outputs = outputs
                    loss1 = criterion(outputs, labels)
                    loss2 = criterion(aux_outputs, labels)
                    loss = loss1 + 0.4 * loss2
                  else:
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                elif params['model'] == 'googlenet':
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                elif params['model'] == 'mobilenet_v2':
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)
                val_loss += loss.item() * inputs.size(0)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        val_loss /= total
        val_acc = correct / total
        print(f'Validation Loss: {val_loss:.4f}, Acc: {val_acc:.4f}')

        wandb.log({'Validation Loss': val_loss, 'Validation Acc': val_acc})

        # Check for early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            no_improve_epoch = 0
            torch.save({
              'epoch': epoch,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'parameters': params
            }, 'model.pth')
        else:
            no_improve_epoch += 1

        if no_improve_epoch >= patience:
            print('Early stopping...')
            break

# Stop carbon tracking, and output the results
tracker.stop()
final_emissions_data = tracker.final_emissions_data
final_emissions = tracker.final_emissions

print("Final emissions data: ", final_emissions_data)
print("Final emissions: ", final_emissions)


# Log to Wandb
wandb.log({"Total emissions": final_emissions})

# End Wandb running
wandb.finish()