# -*- coding: utf-8 -*-
"""è·‘structural_entropy_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PRY56W9wJy62YtesSHS765NTSF01NHuN
"""

import math
import numpy as np
import os
from collections import Counter


def calculate_entropy(x: np.array, chunk_size: int = 256) -> int:
    """
    Calculates the entropy of a given chunk of bytes
    :param x: np.array (Chunk of bytes)
    :param chunk_size: int
    :return: int (entropy of the chunk)
    """
    symbols = Counter(x)
    return sum([-((count/ chunk_size) * math.log2(count/ chunk_size)) for count in symbols.values()])


def convert_to_structural_entropy(bytez_array: np.array, chunk_size: int = 256) -> np.array:
    """
    Converts an executable represented as a 1-d np.array to a stream of entropy values
    :param bytez_list: np.array
    :param chunk_size: int
    :return: np.array (structural entropy of the executable)
    """
    return np.array(
        [calculate_entropy(bytez_array[i:i+chunk_size], chunk_size) for i in range(0, bytez_array.shape[0], chunk_size)]
    )

def main(input_dir, output_dir, chunk_size):
    for filename in os.listdir(input_dir):
        with open(os.path.join(input_dir, filename), 'rb') as f:
            x = f.read()
            x = np.frombuffer(x, dtype=np.uint8).astype(np.int16)

            x = convert_to_structural_entropy(x, chunk_size)
            np.save(os.path.join(output_dir, os.path.splitext(filename)[0]), x)


# Replace the directories and chunk_size below with your actual values
input_dirs = [
    "/content/drive/Shareddrives/BODMAS/BODMAS/ganelp",
    "/content/drive/Shareddrives/BODMAS/BODMAS/sfone",
    "/content/drive/Shareddrives/BODMAS/BODMAS/upatre",
    "/content/drive/Shareddrives/BODMAS/BODMAS/wabot",
    "/content/drive/Shareddrives/BODMAS/BODMAS/wacatac"
]
output_dirs = [
    "/content/drive/MyDrive/BODMAS/ganelp",
    "/content/drive/MyDrive/BODMAS/sfone",
    "/content/drive/MyDrive/BODMAS/upatre",
    "/content/drive/MyDrive/BODMAS/wabot",
    "/content/drive/MyDrive/BODMAS/wacatac"
]
chunk_size = 256

for input_dir, output_dir in zip(input_dirs, output_dirs):
    main(input_dir, output_dir, chunk_size)

import os
import numpy as np
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from torch.nn import functional as F
from torch.autograd import Variable

# check for GPU availability
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device: ", device)

# define model
class CNNClassifier(nn.Module):
    def __init__(self, sequence_size, num_classes):
        super(CNNClassifier, self).__init__()
        self.conv = nn.Conv1d(1, 64, kernel_size=3, padding=1)
        self.gmp = nn.MaxPool1d(sequence_size)
        self.dropout = nn.Dropout(p=0.5)
        self.fc = nn.Linear(64, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.conv(x)
        x = F.relu(x)
        x = self.gmp(x)
        x = x.squeeze(2)
        x = self.dropout(x)
        x = self.fc(x)
        return x

class EntropyDataset(Dataset):
    def __init__(self, files, labels, seq_len):
        self.seq_len = seq_len
        self.files = files
        self.labels = labels

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file = self.files[idx]
        x = np.load(file)
        if len(x) > self.seq_len:
            x = x[:self.seq_len]
        elif len(x) < self.seq_len:
            x = np.pad(x, (0, self.seq_len - len(x)))
        y = self.labels[idx]
        return torch.from_numpy(x).float(), torch.tensor(y, dtype=torch.long)

# parameters
num_classes = 5
num_epochs = 50
batch_size = 8
learning_rate = 0.001
patience = 5
directories = ["/content/drive/MyDrive/BODMAS/ganelp",
    "/content/drive/MyDrive/BODMAS/sfone",
    "/content/drive/MyDrive/BODMAS/upatre",
    "/content/drive/MyDrive/BODMAS/wabot",
    "/content/drive/MyDrive/BODMAS/wacatac"
    ]

label_mapping = {directory.split("/")[-1]: i for i, directory in enumerate(directories)}

files = []
labels = []

min_length = 4096  # Set the minimum sequence length
for directory in directories:
    dir_files = [os.path.join(directory, file) for file in os.listdir(directory) if np.load(os.path.join(directory, file)).shape[0] >= min_length]
    files.extend(dir_files)
    labels.extend([label_mapping[directory.split("/")[-1]]]*len(dir_files))

sequence_size = max([np.load(file).shape[0] for file in files])

train_files, val_files, train_labels, val_labels = train_test_split(files, labels, test_size=0.2, stratify=labels)

train_dataset = EntropyDataset(train_files, train_labels, sequence_size)
val_dataset = EntropyDataset(val_files, val_labels, sequence_size)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# model, loss, and optimizer
model = CNNClassifier(sequence_size, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# initialize early stopping parameters
early_stopping_counter = 0
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_running_loss = 0.0
    train_running_corrects = 0

    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))

        optimizer.zero_grad()

        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        train_running_loss += loss.item()
        train_running_corrects += (preds == labels).sum().item()

    train_loss = train_running_loss/len(train_loader.dataset)
    train_acc = (train_running_corrects/len(train_loader.dataset)) * 100

    model.eval()
    val_running_loss = 0.0
    val_running_corrects = 0

    for i, data in enumerate(val_loader, 0):
        inputs, labels = data
        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))

        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)

        val_running_loss += loss.item()
        val_running_corrects += (preds == labels).sum().item()

    val_loss = val_running_loss/len(val_loader.dataset)
    val_acc = (val_running_corrects/len(val_loader.dataset)) * 100

    print('Epoch: {}/{} | Training Loss: {:.4f} | Training Acc: {:.2f}% | Validation Loss: {:.4f} | Validation Acc: {:.2f}%'.format(
        epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc))

    # early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        if early_stopping_counter >= patience:
            print("Early stopping triggered.")
            break

from google.colab import drive
drive.mount('/content/drive')

sequence_size = max([np.load(file).shape[0] for file in files])
sequence_size

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

import os
import numpy as np
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from torch.nn import functional as F
from torch.autograd import Variable

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

# check for GPU availability
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device: ", device)

# define model
class CNNClassifier(nn.Module):
    def __init__(self, sequence_size, num_classes):
        super(CNNClassifier, self).__init__()

        self.conv1 = nn.Conv1d(1, 50, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(50, 70, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(70, 70, kernel_size=3, padding=1)

        self.gmp = nn.MaxPool1d(sequence_size)
        self.fc1 = nn.Linear(70, 1000)
        self.fc2 = nn.Linear(1000, num_classes)

        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        x = x.unsqueeze(1)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        x = self.gmp(x)
        x = x.squeeze(2)

        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

class EntropyDataset(Dataset):
    def __init__(self, files, labels, seq_len):
        self.seq_len = seq_len
        self.files = files
        self.labels = labels

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file = self.files[idx]
        x = np.load(file)
        if len(x) > self.seq_len:
            x = x[:self.seq_len]
        elif len(x) < self.seq_len:
            x = np.pad(x, (0, self.seq_len - len(x)))
        y = self.labels[idx]
        return torch.from_numpy(x).float(), torch.tensor(y, dtype=torch.long)

# parameters
num_classes = 5
num_epochs = 20
batch_size = 16
learning_rate = 0.01
patience = 3
directories = ["/content/drive/MyDrive/BODMAS/ganelp",
    "/content/drive/MyDrive/BODMAS/sfone",
    "/content/drive/MyDrive/BODMAS/upatre",
    "/content/drive/MyDrive/BODMAS/wabot",
    "/content/drive/MyDrive/BODMAS/wacatac"]

label_mapping = {directory.split("/")[-1]: i for i, directory in enumerate(directories)}

files = []
labels = []

min_length = 2048  # Set the minimum sequence length
for directory in directories:
    dir_files = [os.path.join(directory, file) for file in os.listdir(directory) if np.load(os.path.join(directory, file)).shape[0] >= min_length]
    files.extend(dir_files)
    labels.extend([label_mapping[directory.split("/")[-1]]]*len(dir_files))

sequence_size = max([np.load(file).shape[0] for file in files])

train_files, val_files, train_labels, val_labels = train_test_split(files, labels, test_size=0.2, stratify=labels)

train_dataset = EntropyDataset(train_files, train_labels, sequence_size)
val_dataset = EntropyDataset(val_files, val_labels, sequence_size)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# model, loss, and optimizer
model = CNNClassifier(sequence_size, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# initialize early stopping parameters
early_stopping_counter = 0
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_running_loss = 0.0
    train_running_corrects = 0

    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))

        optimizer.zero_grad()

        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        train_running_loss += loss.item()
        train_running_corrects += (preds == labels).sum().item()

    train_loss = train_running_loss/len(train_loader.dataset)
    train_acc = (train_running_corrects/len(train_loader.dataset)) * 100

    model.eval()
    val_running_loss = 0.0
    val_running_corrects = 0

    for i, data in enumerate(val_loader, 0):
        inputs, labels = data
        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))

        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)

        val_running_loss += loss.item()
        val_running_corrects += (preds == labels).sum().item()

    val_loss = val_running_loss/len(val_loader.dataset)
    val_acc = (val_running_corrects/len(val_loader.dataset)) * 100

    print('Epoch: {}/{} | Training Loss: {:.4f} | Training Acc: {:.2f}% | Validation Loss: {:.4f} | Validation Acc: {:.2f}%'.format(
        epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc))

    # early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        if early_stopping_counter >= patience:
            print("Early stopping triggered.")
            break

import os
import numpy as np
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from torch.nn import functional as F
from torch.autograd import Variable

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

# check for GPU availability
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device: ", device)

# define model
class FCNClassifier(nn.Module):
    def __init__(self, sequence_size, num_classes):
        super(FCNClassifier, self).__init__()

        self.conv1 = nn.Conv1d(1, 128, kernel_size=8, padding=3)  # Padding=(8-1)/2 for 'same' padding
        self.conv2 = nn.Conv1d(128, 256, kernel_size=5, padding=2)  # Padding=(5-1)/2
        self.conv3 = nn.Conv1d(256, 128, kernel_size=3, padding=1)  # Padding=(3-1)/2

        self.gap = nn.AdaptiveAvgPool1d(1)  # This will make sure the output is 1 in the time dimension
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)  # (batch_size, 1, sequence_size)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        x = self.gap(x)
        x = x.squeeze(-1)  # Remove the time dimension

        x = self.fc(x)
        return x


class EntropyDataset(Dataset):
    def __init__(self, files, labels, seq_len):
        self.seq_len = seq_len
        self.files = files
        self.labels = labels

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file = self.files[idx]
        x = np.load(file)
        if len(x) > self.seq_len:
            x = x[:self.seq_len]
        elif len(x) < self.seq_len:
            x = np.pad(x, (0, self.seq_len - len(x)))
        y = self.labels[idx]
        return torch.from_numpy(x).float(), torch.tensor(y, dtype=torch.long)

# parameters
num_classes = 5
num_epochs = 20
batch_size = 4
learning_rate = 0.01
patience = 3
directories = ["/content/drive/MyDrive/BODMAS/ganelp",
    "/content/drive/MyDrive/BODMAS/sfone",
    "/content/drive/MyDrive/BODMAS/upatre",
    "/content/drive/MyDrive/BODMAS/wabot",
    "/content/drive/MyDrive/BODMAS/wacatac"]

label_mapping = {directory.split("/")[-1]: i for i, directory in enumerate(directories)}

files = []
labels = []

min_length = 2048  # Set the minimum sequence length
for directory in directories:
    dir_files = [os.path.join(directory, file) for file in os.listdir(directory) if np.load(os.path.join(directory, file)).shape[0] >= min_length]
    files.extend(dir_files)
    labels.extend([label_mapping[directory.split("/")[-1]]]*len(dir_files))

sequence_size = max([np.load(file).shape[0] for file in files])

train_files, val_files, train_labels, val_labels = train_test_split(files, labels, test_size=0.2, stratify=labels)

train_dataset = EntropyDataset(train_files, train_labels, sequence_size)
val_dataset = EntropyDataset(val_files, val_labels, sequence_size)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# model, loss, and optimizer
model = FCNClassifier(sequence_size, num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# initialize early stopping parameters
early_stopping_counter = 0
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_running_loss = 0.0
    train_running_corrects = 0

    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))

        optimizer.zero_grad()

        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        train_running_loss += loss.item()
        train_running_corrects += (preds == labels).sum().item()

    train_loss = train_running_loss/len(train_loader.dataset)
    train_acc = (train_running_corrects/len(train_loader.dataset)) * 100

    model.eval()
    val_running_loss = 0.0
    val_running_corrects = 0

    for i, data in enumerate(val_loader, 0):
        inputs, labels = data
        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))

        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        loss = criterion(outputs, labels)

        val_running_loss += loss.item()
        val_running_corrects += (preds == labels).sum().item()

    val_loss = val_running_loss/len(val_loader.dataset)
    val_acc = (val_running_corrects/len(val_loader.dataset)) * 100

    print('Epoch: {}/{} | Training Loss: {:.4f} | Training Acc: {:.2f}% | Validation Loss: {:.4f} | Validation Acc: {:.2f}%'.format(
        epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc))

    # early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        if early_stopping_counter >= patience:
            print("Early stopping triggered.")
            break